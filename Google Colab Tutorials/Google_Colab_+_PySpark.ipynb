{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ad17171717/YouTube-Tutorials/blob/main/Google%20Colab%20Tutorials/Google_Colab_%2B_PySpark.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "191HB-DSMdSM"
      },
      "source": [
        "# **PySpark**\n",
        "\n",
        "**PySpark is the Python API for Apache Spark. It enables users to perform real-time, large-scale data processing in a distributed environment using Python. PySpark combines Python’s learnability and ease of use with the power of Apache Spark to enable processing and analysis of data at any size for everyone familiar with Python. PySpark supports all of Spark’s features such as Spark SQL, DataFrames, Structured Streaming, Machine Learning (MLlib) and Spark Core.**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<sup>Source: [PySpark Documentation](https://spark.apache.org/docs/latest/api/python/index.html) from Apache.org</sup>"
      ],
      "metadata": {
        "id": "v3_B_y3uAt-V"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ith8T3JPYJGo"
      },
      "source": [
        "## **Installing PySpark**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#check that java is installed\n",
        "!java -version"
      ],
      "metadata": {
        "id": "Lb9P1hpLCaC3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "113d57ba-05e0-47e8-9fab-9dd3e8dcbe98"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "openjdk version \"11.0.23\" 2024-04-16\n",
            "OpenJDK Runtime Environment (build 11.0.23+9-post-Ubuntu-1ubuntu122.04.1)\n",
            "OpenJDK 64-Bit Server VM (build 11.0.23+9-post-Ubuntu-1ubuntu122.04.1, mixed mode, sharing)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "rqIekXhmYM_X",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a550f63e-9947-4e26-aca6-2605beb5e761"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyspark\n",
            "  Downloading pyspark-3.5.1.tar.gz (317.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.0/317.0 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "Building wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.5.1-py2.py3-none-any.whl size=317488491 sha256=3d038ba9f5b5b2a1c298b8f1505a03f29c9684d0dc59134724a8d85a447de6cc\n",
            "  Stored in directory: /root/.cache/pip/wheels/80/1d/60/2c256ed38dddce2fdd93be545214a63e02fbd8d74fb0b7f3a6\n",
            "Successfully built pyspark\n",
            "Installing collected packages: pyspark\n",
            "Successfully installed pyspark-3.5.1\n"
          ]
        }
      ],
      "source": [
        "#install pyspark\n",
        "!pip install pyspark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "UBf_r69QXSWo"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType, LongType, TimestampType"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yF_a0auLYRI_"
      },
      "source": [
        "## **Working with PySpark**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i0g4htAdZr-X"
      },
      "source": [
        "### **Download Dataset**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZluUVYeOX2TX"
      },
      "outputs": [],
      "source": [
        "#use curl to download the data then unzip it into the content directory\n",
        "!curl -O https://datarepo.eng.ucsd.edu/mcauley_group/data/amazon_2023/raw/review_categories/Automotive.jsonl.gz\n",
        "!gunzip -f Automotive.jsonl.gz"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MBWacreZ8BNR"
      },
      "source": [
        "<sup>Data Source: [Amazon Reviews](https://amazon-reviews-2023.github.io/#) from UCSD by Hou, Yupeng and Li, Jiacheng and He, Zhankui and Yan, An and Chen, Xiusi and McAuley, Julian</sup>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AENSsTTOg--C"
      },
      "outputs": [],
      "source": [
        "print(f'The size of the Amazon Automotive Revie file is: {os.path.getsize(\"/content/Automotive.jsonl\") / (1024 ** 3):.2f} GB')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-z6xk1MsjHIC"
      },
      "source": [
        "### **Initializing a SparkSession**\n",
        "\n",
        "**A `SparkSession` is a class from the PySpark module. It is the entrypoint to working with Apache Spark. This will allow us to create a PySpark DataFrame**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<sup>Source: [pyspark.sql.SparkSession](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.SparkSession.html) from Spark.Apache.org</sup>"
      ],
      "metadata": {
        "id": "RXUGkoEi6qTn"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "oHmxoDNFjHZi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3de28056-f371-4305-82f3-2eaeb89d3794"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The Spark version is 3.5.1\n"
          ]
        }
      ],
      "source": [
        "spark = SparkSession.builder.appName('AutomotiveData').getOrCreate()\n",
        "\n",
        "print(f'The Spark version is {spark.version}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fx2XwRyseVs4"
      },
      "source": [
        "### **Reading in the Data**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V6iwjxsigwnB"
      },
      "source": [
        "#### **pandas**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NQJUBKKSeF4p"
      },
      "outputs": [],
      "source": [
        "#attempting to read a 8 GB file into a pandas DataFrame will cause the session to crash\n",
        "pandas_df = pd.read_json('/content/Automotive.jsonl', lines=True)\n",
        "print(pandas_df.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L3I3a2rOh6wC"
      },
      "source": [
        "#### **PySpark**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Using a Schema for a PySpark DataFrame**\n",
        "\n",
        "**Within Apache Spark a `schema` provides the data format for a DataFrame or a Dataset.**\n",
        "\n",
        "**For our `schema`, we will first set our `StructType` which is a set of `StructField`s. A `StructField` contains information for a given column including the column name (`name`), the type of data within the rows (`dataType`) and whether or not the row can contain a null value (`nullable`).**"
      ],
      "metadata": {
        "id": "j7-vS6rY7bR3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1a0NFeD5O5f-"
      },
      "outputs": [],
      "source": [
        "schema = StructType([\n",
        "    StructField(\"asin\", StringType(), True),\n",
        "    StructField(\"helpful_vote\", IntegerType(), True),\n",
        "    StructField(\"images\", StringType(), True),\n",
        "    StructField(\"parent_asin\", StringType(), True),\n",
        "    StructField(\"rating\", DoubleType(), True),\n",
        "    StructField(\"text\", StringType(), True),\n",
        "    StructField(\"timestamp\", TimestampType(), True),\n",
        "    StructField(\"title\", StringType(), True),\n",
        "    StructField(\"user_id\", StringType(), True),\n",
        "    StructField(\"verified_purchase\", StringType(), True)\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s_CKp_kSci8T"
      },
      "outputs": [],
      "source": [
        "spark_df = spark.read.schema(schema).json('/content/Automotive.jsonl')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B2e7L4timf06"
      },
      "source": [
        "### **PySpark Operations**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "em3LyWUImyqL"
      },
      "outputs": [],
      "source": [
        "#display first 5 rows of the pyspark DataFrame\n",
        "print(spark_df.show(5))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S3lnhfBWm0Lr"
      },
      "outputs": [],
      "source": [
        "#print the columns and datatypes of the DataFrame\n",
        "spark_df.printSchema()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wc-z7HpWu3B2"
      },
      "outputs": [],
      "source": [
        "#retrieve summary statistics for the DataFrame\n",
        "spark_df.describe().show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-3JSiaHuvEGN"
      },
      "outputs": [],
      "source": [
        "#retrieve value counts for the rating column\n",
        "spark_df.groupBy('rating').count().show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s6s10rjofSpZ"
      },
      "source": [
        "## **Limitations of PySpark within Google Colab**\n",
        "\n",
        "**Google Colab provides a platform to Google Account holders for no additional monetary fee; because of this resources within Google Colab are limited. Additionally the computation will be slower because of cloud latency. Within the tutorial we saw how long it can take to read a 8 GB file into a PySpark DataFrame or running calculations within a Google Colab session. For the free edition of Google Colab using PySpark can be great for learning PySpark or working with early stage prototype scripts. However, the free version of Google Colab should not be used for development or productions purposes.**\n",
        "\n",
        "**Consider working with datasets 2GB or smaller in Google Colab when using PySpark.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8bryll2djqgi"
      },
      "source": [
        "<sup>Source: [Google Colab Frequently Asked Questions](https://research.google.com/colaboratory/faq.html)</sup>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yKNxQRszk3mr"
      },
      "source": [
        "# **References and Additional Learning**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I09itkOt8koZ"
      },
      "source": [
        "## **Data**\n",
        "\n",
        "- **[Amazon Reviews](https://amazon-reviews-2023.github.io/#) from UCSD by Hou, Yupeng and Li, Jiacheng and He, Zhankui and Yan, An and Chen, Xiusi and McAuley, Julian**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZY0UK87Evct9"
      },
      "source": [
        "## **Documentation**\n",
        "\n",
        "- **[Google Colab Frequently Asked Questions](https://research.google.com/colaboratory/faq.html) from research.google.com**\n",
        "\n",
        "- **[PySpark Documentation](https://spark.apache.org/docs/latest/api/python/index.html) from Apache.org**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mf_fVqj5k4PT"
      },
      "source": [
        "# **Connect**\n",
        "- **Feel free to connect with Adrian on [YouTube](https://www.youtube.com/channel/UCPuDxI3xb_ryUUMfkm0jsRA), [LinkedIn](https://www.linkedin.com/in/adrian-dolinay-frm-96a289106/), [X](https://twitter.com/DolinayG), [GitHub](https://github.com/ad17171717), [Medium](https://adriandolinay.medium.com/) and [Odysee](https://odysee.com/@adriandolinay:0). Happy coding!**"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}